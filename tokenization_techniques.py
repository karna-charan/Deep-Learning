# -*- coding: utf-8 -*-
"""Tokenization Techniques.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dUsVoEWNcCbgPngNHbDtawkqwyi5sqdz

**Whitespace Tokenization**
"""

# Take input text from user
text = input("Enter text: ")
# Split text based on spaces
tokens = text.split()
# Display tokens
print("Whitespace Tokens:", tokens)

"""**Word Tokenization (NLP Library â€“ NLTK)**"""

import nltk
nltk.download('punkt')
nltk.download('punkt_tab')
from nltk.tokenize import word_tokenize
# Take input text from user
text = input("Enter text: ")
# Tokenize text into words using NLTK
tokens = word_tokenize(text)
# Display tokens
print("Word Tokens:", tokens)

"""**Character-Level Tokenization**"""

# Take input text from user
text = input("Enter text: ")
# Convert text into character-level tokens
tokens = list(text)
# Display tokens
print("Character Tokens:", tokens)

"""**Sentence Tokenization**"""

import nltk
nltk.download('punkt')
nltk.download('punkt_tab')
from nltk.tokenize import sent_tokenize
# Take input text from user
text = input("Enter text: ")
# Tokenize text into sentences
sentences = sent_tokenize(text)
# Display sentences
print("Sentence Tokens:")
for s in sentences:
    print(s)

"""**Stopword Removal**"""

import nltk
nltk.download('stopwords')
nltk.download('punkt')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
# Take input text from user
text = input("Enter text: ")
# Tokenize text into words
words = word_tokenize(text)
# Load English stopwords
stop_words = set(stopwords.words('english'))
# Remove stopwords
filtered_words = [word for word in words if word.lower() not in stop_words]
# Display results
print("Original Tokens:", words)
print("After Stopword Removal:", filtered_words)

"""**Subword Tokenization**"""

from transformers import AutoTokenizer
# Load BERT tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
# Take input text from user
text = input("Enter text: ")
# Perform subword tokenization
tokens = tokenizer.tokenize(text)
# Convert tokens to token IDs
token_ids = tokenizer.encode(text)
# Display tokens and IDs
print("Subword Tokens:", tokens)
print("Token IDs:", token_ids)

"""**1B. Text Embedding Technique**

**One-Hot Encoding:**
"""

from sklearn.preprocessing import OneHotEncoder
# Take input sentence
text = input("Enter a sentence: ")
# Split sentence into words
words = text.lower().split()
# Initialize One-Hot Encoder
encoder = OneHotEncoder(sparse_output=False)
# Fit and transform words
one_hot = encoder.fit_transform([[w] for w in words])
# Display result
print("Words:", words)
print("One-Hot Encoding:")
print(one_hot)

"""**Bag of Words**"""

from sklearn.feature_extraction.text import CountVectorizer
# Take input sentence
text = input("Enter a sentence: ")
# Create CountVectorizer object
vectorizer = CountVectorizer()
# Generate BoW representation
bow = vectorizer.fit_transform([text])
# Display vocabulary and vector
print("Vocabulary:", vectorizer.get_feature_names_out())
print("BoW Vector:")
print(bow.toarray())

"""**. Word2Vec:**"""

!pip install gensim

from gensim.models import Word2Vec
import re
# Take input sentence
text = input("Enter a sentence: ")
# Tokenize sentence using regex
tokens = re.findall(r'\b\w+\b', text.lower())
# Train Word2Vec model
model = Word2Vec(
    [tokens],
    vector_size=50,
    window=3,
    min_count=1,
    sg=1
)
# Display embeddings for each word
print("Word2Vec Embeddings:")
for word in tokens:
    print(word, "->", model.wv[word])

"""**GloVe Embedding**"""

import gensim.downloader as api
import re
# Load pre-trained GloVe model (50-dimensional)
glove_model = api.load("glove-wiki-gigaword-50")
# Take input sentence
text = input("Enter a sentence: ")
# Tokenize sentence
tokens = re.findall(r'\b\w+\b', text.lower())
# Display embeddings
print("GloVe Embeddings:")
for word in tokens:
    if word in glove_model:
        print(word, "->", glove_model[word])
    else:
        print(word, "-> Not in vocabulary")

"""**BERT Embedding:**"""

import torch
from transformers import BertTokenizer, BertModel
# Load BERT tokenizer and model
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased")

# Take input sentence
text = input("Enter a sentence: ")
# Convert sentence into BERT input format
inputs = tokenizer(text, return_tensors="pt")
# Get model outputs
outputs = model(**inputs)
# Extract CLS token embedding
cls_embedding = outputs.last_hidden_state[:, 0, :]
# Display embedding
print("BERT CLS Embedding Shape:", cls_embedding.shape)
print("BERT Embedding:")
print(cls_embedding)