# -*- coding: utf-8 -*-
"""Tokenization1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tqcb1R9xEcZFlatj5AEPRW4FvdVIeVp4

**Whitespace Tokenization**
"""

text = "i am in alliance university"
tokens = text.split()
print(tokens)

"""**Character-Level Tokenization**"""

text = "karna charan"
tokens = []

for char in text:
    tokens.append(char)

print(tokens)

"""**Sub Word Tokenization**"""

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

text = "unhappiness"
tokens = tokenizer.tokenize(text)

print(tokens)

"""**TOKENIZATION WITH STOP WORD REMOVAL**"""

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Download required resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('punkt_tab') # Added to fix the LookupError

text = "Natural Language Processing is very useful in artificial intelligence"

# Tokenization
tokens = word_tokenize(text)

# Stop word removal
stop_words = set(stopwords.words('english'))
filtered_tokens = [word for word in tokens if word.lower() not in stop_words]

print(filtered_tokens)

"""**Sentence Tokenization and Word Tokenization**"""

import nltk
from nltk.tokenize import sent_tokenize

nltk.download('punkt')

text = "Natural Language Processing is important. It is widely used in AI."

sentences = sent_tokenize(text)
print(sentences)

import nltk
from nltk.tokenize import word_tokenize

nltk.download('punkt')

text = "Natural Language Processing is important"

words = word_tokenize(text)
print(words)

"""**NLP Libraries Using Tokenization**

**NLTK Tokenization**
"""

import nltk
from nltk.tokenize import sent_tokenize

nltk.download('punkt')

text = "NLP is interesting. It is used in AI."
sentences = sent_tokenize(text)

print(sentences)

"""**Hugging Face Tokenizer**"""

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

text = "tokenization"
tokens = tokenizer.tokenize(text)

print(tokens)

